{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('GIS712': conda)",
   "display_name": "Python 3.8.5 64-bit ('GIS712': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ff0c070e84440430e358ec0281581361f68081b25da57249b48dc9fe569643ae"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## we define the product type here, Landsat 5:\"CU_LT05.001\", Landsat 7 :\"CU_LE07.001\", Landsat 8 :\"CU_LC08.001\"\n",
    "prods = [\"CU_LT05.001\"]\n",
    "layers = [(prods[0], \"PIXELQA\"), (prods[0], \"SRB1\"), (prods[0], \"SRB2\"), (prods[0], \"SRB3\"), (prods[0], \"SRB4\"), (prods[0], \"SRB5\"), (prods[0], \"SRB7\"), (prods[0], \"SRCLOUDQA\")]\n",
    "prodLayer = []\n",
    "for l in layers:\n",
    "    prodLayer.append({\n",
    "        \"layer\": l[1],\n",
    "        \"product\": l[0]\n",
    "    })\n",
    "prodLayer"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'layer': 'PIXELQA', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB1', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB2', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB3', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB4', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB5', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRB7', 'product': 'CU_LT05.001'},\n",
       " {'layer': 'SRCLOUDQA', 'product': 'CU_LT05.001'}]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'token_type': 'Bearer',\n",
       " 'token': 'NFLiWmOmZWaO7zcS6NI9naRyeO2qHCYALGgeYHCPrM3Gr6zlXg1F_Yec3p9WgqiRst1HP9q7uy3coQ1EbL5ivg',\n",
       " 'expiration': '2020-10-14T15:39:49Z'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "## establish credendtial for the api\n",
    "import requests as r\n",
    "import getpass, pprint, time, os, cgi, json\n",
    "import geopandas as gpd\n",
    "\n",
    "## Enter Earth Data login Credentials\n",
    "username = \"ZekunLin\"#getpass.getpass('Earthdata Username:')\n",
    "password = \"P6Fn8T47\"#getpass.getpass('Earthdata Password:')\n",
    "\n",
    "api = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  # Set the AρρEEARS API to a variable\n",
    "\n",
    "token_response = r.post('{}login'.format(api), auth=(username, password)).json() # Insert API URL, call login service, provide credentials & return json\n",
    "del username, password                                                           # Remove user and password information\n",
    "token_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['native',\n",
       " 'geographic',\n",
       " 'sinu_modis',\n",
       " 'albers_weld_alaska',\n",
       " 'albers_weld_conus',\n",
       " 'albers_ard_alaska',\n",
       " 'albers_ard_conus',\n",
       " 'albers_ard_hawaii',\n",
       " 'easegrid_2_global',\n",
       " 'easegrid_2_north',\n",
       " 'laea_sphere_19']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# look up the available projections \n",
    "projections = r.get('{}spatial/proj'.format(api)).json()  # Call to spatial API, return projs as json\n",
    "projections\n",
    "projs = {}\n",
    "for p in projections:\n",
    "    projs[p[\"Name\"]] = p\n",
    "list(projs.keys())\n",
    "\n"
   ]
  },
  {
   "source": [
    "## submit the orders by tiles\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "tile_list = glob.glob(\"D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles/*.shp\")      # where the 75 tiles stored\n",
    "short_name = [os.path.basename(x) for x in glob.glob(\"D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles/*.shp\")]  # get the tile names\n",
    "\n",
    "period1 = [\"01-01-1984\", \"12-31-1999\"]                  # due to limits of APPEEARS' data process capability we need to download data by 2 periods\n",
    "period2 = [\"01-01-2000\", \"12-31-2013\"]\n",
    "periods = {\"1\" : period1, \"2\" : period2}\n",
    "\n",
    "for p in [\"1\", \"2\"]:\n",
    "\n",
    "    for i in range(len(tile_list)):\n",
    "\n",
    "        nps = gpd.read_file(tile_list[i])\n",
    "        nps = nps.to_crs(\"EPSG:4326\")\n",
    "\n",
    "        nps_gc = nps.to_json()\n",
    "        nps_gc = json.loads(nps_gc)\n",
    "\n",
    "        task_name = \"p\" + p + \"_\" + short_name[i][0:7]        # task name = period + tile name\n",
    "        task_type = \"area\"                                    # we are using polygon instead of points to extract data\n",
    "        proj = projs[\"geographic\"][\"Name\"]                    # it MUST BE geographic projection make this api work\n",
    "        outFormat = [\"geotiff\"]\n",
    "        startDate = periods[p][0]\n",
    "        endDate = periods[p][1]\n",
    "        recurring = False\n",
    "\n",
    "        task = {                                # build a json type of geo area to indicate ROI\n",
    "            \"task_type\": task_type,             # area sample task\n",
    "            \"task_name\": task_name,\n",
    "            \"params\": {\n",
    "                \"dates\": [\n",
    "                {\n",
    "                    \"startDate\": startDate,\n",
    "                    \"endDate\": endDate\n",
    "                }],\n",
    "                \"layers\": prodLayer,\n",
    "                \"output\": {\n",
    "                    \"format\": {\n",
    "                        \"type\": outFormat[0]    # geotiff format\n",
    "                    },\n",
    "                    \"projection\": proj\n",
    "                },\n",
    "                \"geo\": nps_gc\n",
    "                }\n",
    "        }\n",
    "\n",
    "\n",
    "        token = token_response['token']                      # Save login token to a variable\n",
    "        head = {'Authorization': 'Bearer {}'.format(token)}  # Create a header to store token information, needed to submit a request\n",
    "\n",
    "        task_response = r.post(\"{}task\".format(api), json = task, headers = head).json()\n",
    "        print(task_response)\n",
    "        \n",
    "        task_id = task_response['task_id']                                               # Set task id from request submission\n",
    "        status_response = r.get('{}status/{}'.format(api, task_id), headers=head).json() # Call status service with specific task ID & user credentials\n",
    "        print(status_response['status'])\n",
    "\n",
    "        write_task_id = task_id + \" : \" + task_name + \":\" + status_response['status']\n",
    "        t = open(os.path.join(\"D:/Zekun/Landsat_ARD/Landsat_ARD/se_ard/\", \"LT5_task_id_by_tile.txt\"), 'a')\n",
    "        t.write(write_task_id +\"\\n\")\n",
    "        t.close()\n",
    "\n",
    "        print('orders have been successfully submitted.')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'task_id': 'a3507b50-bd6a-4c61-8f84-b87adeca7401', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'feffeca6-a8a2-4965-a455-a06fb9929ca4', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '968326fa-7a0d-48f9-8682-aca4af6e35a1', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'e29a2e8c-e62b-449e-af2f-ccd1e0159113', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '8d10d518-e4cf-4f08-b12b-ce364e4589e6', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '2a81ed00-ddaf-49f9-88d1-92999ff80ad7', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'ebd6823c-755d-416e-b410-607fbc410169', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '0890126c-5ac1-46b1-89b3-1caa904dcf07', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'cde084e3-ddb2-43a4-8459-15c0f6e8e6cb', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'b9788e0e-beac-4760-a714-8d3f69297aff', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '965264ab-d337-4d51-a91f-85f9849a7bf2', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'b2906c1e-8f45-4e03-91e3-c51a84042c5b', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '5c9d7515-6fbf-4c8d-82f9-a68b09d1f2f2', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '2df09a32-2f4c-4e37-a56d-1f7c8011b4f9', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '24b386ef-75cc-4794-b3f0-f2cf90cae3c6', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '427d0117-1cdd-44b5-a0ea-2445a34ea814', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '016f1504-4c9b-4ded-9128-b8ba011ec460', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'a1cdd24d-958e-420e-837c-48f18d668a69', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '30b47a0f-f852-4658-9780-caf4666ad1bf', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '20108dd2-d170-4361-9fa3-4737f4206629', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '953b38dc-2a3d-4390-bb30-9fa0fdc0eeda', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'd59eb272-99d2-427a-b94d-23421184b794', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '552e4814-204a-4c18-a96c-a02768962807', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': 'b434a350-45d2-44d6-99fd-cdf8d5775023', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '9131ae94-9d86-4f99-94ec-d79e809736e6', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '881b114a-c193-4ba7-96e1-4ee425aa91c0', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '6da943db-4825-4ead-83e1-31287de565a3', 'status': 'pending'}\n",
      "pending\n",
      "{'task_id': '07bd61ed-7097-471d-907f-f597eb3e4abf', 'status': 'pending'}\n",
      "pending\n",
      "{'message': 'You have exceeded the maximum number of requests allowed per day.  Please try again tomorrow.'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'task_id'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2daa3a4ed03b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mtask_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'task_id'\u001b[0m\u001b[1;33m]\u001b[0m                                               \u001b[1;31m# Set task id from request submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mstatus_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}status/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call status service with specific task ID & user credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'task_id'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloaded files can be found at: D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\\CU_LT05.001\\test_1\n"
     ]
    }
   ],
   "source": [
    "## we start to download the data if they have been processed\n",
    "import requests as r\n",
    "import getpass, pprint, time, os, cgi, json\n",
    "\n",
    "token = token_response['token']                                                     # retrieve the existed orders in the Appeears\n",
    "response = r.get('https://lpdaacsvc.cr.usgs.gov/appeears/api/task', \n",
    "    headers={'Authorization': 'Bearer {0}'.format(token)})\n",
    "task_response = response.json()\n",
    "\n",
    "t = open(\"D:/Zekun/Landsat_ARD/Landsat_ARD/se_ard/LT5_task_id_by_tile.txt\", \"r\")    # read the list of orders\n",
    "task_id_list = t.read().split('\\n')\n",
    "\n",
    "for x in range(len(task_id_list)):\n",
    "    submited_id = task_id_list[x][0:36]\n",
    "    for y in range(len(task_response)):\n",
    "        if task_response[y]['task_id'] == submited_id:\n",
    "            if task_response[y]['status'] == 'done':\n",
    "                download_this_name = task_response[y]['task_name']\n",
    "                download_this_id = task_response[y]['task_id']\n",
    "                bundle = r.get('{}bundle/{}'.format(api, download_this_id)).json() # Call API and return bundle contents for the task_id as json\n",
    "                #print(bundle['files'])\n",
    "                files = {}                                                         # Create empty dictionary\n",
    "                for f in bundle['files']: files[f['file_id']] = f['file_name']    # Fill dictionary with file_id as keys and file_name as values\n",
    "\n",
    "                destDir = os.path.join(\"D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\", prods[0], download_this_name)\n",
    "                if not os.path.exists(destDir) : os.makedirs(destDir)\n",
    "\n",
    "                for f in files:\n",
    "                    dl = r.get('{}bundle/{}/{}'.format(api, download_this_id, f), stream = True)                      # Get a stream to the bundle file\n",
    "                    filename = os.path.basename(cgi.parse_header(dl.headers['Content-Disposition'])[1]['filename'])   # Parse the name from Content-Disposition header\n",
    "                    filepath = os.path.join(destDir, filename)                                                        # Create output file path\n",
    "                    with open(filepath, 'wb') as f:                                                                   # Write file to dest dir\n",
    "                        for data in dl.iter_content(chunk_size=8192): f.write(data)\n",
    "                print('Downloaded files can be found at: {}'.format(destDir))\n",
    "\n",
    "    else: pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'D:\\\\Zekun\\\\Landsat_ARD\\\\Landsat_ARD\\\\se_ard\\\\CU_LT05.001\\\\p1_h22_v10'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "token = token_response['token']                                                     # retrieve the existed orders in the Appeears\n",
    "response = r.get('https://lpdaacsvc.cr.usgs.gov/appeears/api/task', \n",
    "    headers={'Authorization': 'Bearer {0}'.format(token)})\n",
    "task_response = response.json()\n",
    "\n",
    "download_this_name = task_response[1]['task_name']\n",
    "os.path.join(\"D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\", prods[0], download_this_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' bundle = r.get(\\'{}bundle/{}\\'.format(api,task_id)).json()  # Call API and return bundle contents for the task_id as json\\n\\n\\nfiles = {}                                                       # Create empty dictionary\\nfor f in bundle[\\'files\\']: files[f[\\'file_id\\']] = f[\\'file_name\\']   # Fill dictionary with file_id as keys and file_name as values\\n\\n\\ndestDir = os.path.join(\"D:\\\\Zekun\\\\Landsat_ARD\\\\Landsat_ARD\\\\se_ard\\\\CU_LT05.001\", task_name)\\nif not os.path.exists(destDir) : os.makedirs(destDir)\\n\\nfor f in files:\\n    dl = r.get(\\'{}bundle/{}/{}\\'.format(api, task_id, f), stream=True)           # Get a stream to the bundle file\\n    filename = os.path.basename(cgi.parse_header(dl.headers[\\'Content-Disposition\\'])[1][\\'filename\\'])  # Parse the name from Content-Disposition header \\n    filepath = os.path.join(destDir, filename)                                                       # Create output file path\\n    with open(filepath, \\'wb\\') as f:                                                                  # Write file to dest dir\\n        for data in dl.iter_content(chunk_size=8192): f.write(data) \\nprint(\\'Downloaded files can be found at: {}\\'.format(destDir)) '"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "\n",
    "bundle = r.get('{}bundle/{}'.format(api,task_id)).json()  # Call API and return bundle contents for the task_id as json\n",
    "\n",
    "\n",
    "files = {}                                                       # Create empty dictionary\n",
    "for f in bundle['files']: files[f['file_id']] = f['file_name']   # Fill dictionary with file_id as keys and file_name as values\n",
    "\n",
    "\n",
    "destDir = os.path.join(\"D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\\CU_LT05.001\", task_name)\n",
    "if not os.path.exists(destDir) : os.makedirs(destDir)\n",
    "\n",
    "for f in files:\n",
    "    dl = r.get('{}bundle/{}/{}'.format(api, task_id, f), stream=True)           # Get a stream to the bundle file\n",
    "    filename = os.path.basename(cgi.parse_header(dl.headers['Content-Disposition'])[1]['filename'])  # Parse the name from Content-Disposition header \n",
    "    filepath = os.path.join(destDir, filename)                                                       # Create output file path\n",
    "    with open(filepath, 'wb') as f:                                                                  # Write file to dest dir\n",
    "        for data in dl.iter_content(chunk_size=8192): f.write(data) \n",
    "print('Downloaded files can be found at: {}'.format(destDir)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pending'"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "t = open(\"D:/Zekun/Landsat_ARD/Landsat_ARD/se_ard/LT5_task_id_by_tile.txt\", \"r\")\n",
    "task_id_list = t.read().split('\\n')\n",
    "task_id = task_id_list[0][0:36]\n",
    "status_response = r.get('{}status/{}'.format(api, task_id), headers=head).json() # Call status service with specific task ID & user credentials\n",
    "status_response['status']"
   ]
  }
 ]
}