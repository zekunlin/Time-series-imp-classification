{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('GIS712': conda)",
   "display_name": "Python 3.8.5 64-bit ('GIS712': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ff0c070e84440430e358ec0281581361f68081b25da57249b48dc9fe569643ae"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## we define the product type here, Landsat 5:\"CU_LT05.001\", Landsat 7 :\"CU_LE07.001\", Landsat 8 :\"CU_LC08.001\"\n",
    "prods = [\"CU_LE07.001\"]\n",
    "layers = [(prods[0], \"PIXELQA\"), (prods[0], \"SRB1\"), (prods[0], \"SRB2\"), (prods[0], \"SRB3\"), (prods[0], \"SRB4\"), (prods[0], \"SRB5\"), (prods[0], \"SRB7\")]\n",
    "prodLayer = []\n",
    "for l in layers:\n",
    "    prodLayer.append({\n",
    "        \"layer\": l[1],\n",
    "        \"product\": l[0]\n",
    "    })\n",
    "prodLayer"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'layer': 'PIXELQA', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB1', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB2', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB3', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB4', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB5', 'product': 'CU_LE07.001'},\n",
       " {'layer': 'SRB7', 'product': 'CU_LE07.001'}]"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'token_type': 'Bearer',\n",
       " 'token': 'qsqr4jrhL-lVTHq4OFnMy5Wa0tCrScp7Zj_wOmYeRu4QnjIODKWYKUPY9MlMyWlUJXRiT_0KkU2NVqQnQm-U8w',\n",
       " 'expiration': '2020-10-15T14:30:21Z'}"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "## establish credendtial for the api\n",
    "import requests as r\n",
    "import getpass, pprint, time, os, cgi, json\n",
    "import geopandas as gpd\n",
    "\n",
    "## Enter Earth Data login Credentials\n",
    "username = \"lin_ncsu\"#getpass.getpass('Earthdata Username:')\n",
    "password = \"skv6W]j24\"#getpass.getpass('Earthdata Password:')\n",
    "\n",
    "api = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  # Set the AρρEEARS API to a variable\n",
    "\n",
    "token_response = r.post('{}login'.format(api), auth=(username, password)).json() # Insert API URL, call login service, provide credentials & return json\n",
    "del username, password                                                           # Remove user and password information\n",
    "token_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['native',\n",
       " 'geographic',\n",
       " 'sinu_modis',\n",
       " 'albers_weld_alaska',\n",
       " 'albers_weld_conus',\n",
       " 'albers_ard_alaska',\n",
       " 'albers_ard_conus',\n",
       " 'albers_ard_hawaii',\n",
       " 'easegrid_2_global',\n",
       " 'easegrid_2_north',\n",
       " 'laea_sphere_19']"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "# look up the available projections \n",
    "projections = r.get('{}spatial/proj'.format(api)).json()  # Call to spatial API, return projs as json\n",
    "projections\n",
    "projs = {}\n",
    "for p in projections:\n",
    "    projs[p[\"Name\"]] = p\n",
    "list(projs.keys())\n",
    "\n"
   ]
  },
  {
   "source": [
    "## submit the orders by tiles\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "tile_list = glob.glob(\"D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles/*.shp\")      # where the 75 tiles stored\n",
    "short_name = [os.path.basename(x) for x in glob.glob(\"D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles/*.shp\")]  # get the tile names\n",
    "\n",
    "period1 = [\"01-01-1999\", \"12-31-2004\"]            # due to limits of APPEEARS' data process capability we need to download data by 2 periods\n",
    "period2 = [\"01-01-2005\", \"12-31-2010\"]\n",
    "period3 = [\"01-01-2011\", \"12-31-2016\"]\n",
    "period4 = [\"01-01-2017\", \"12-31-2018\"]\n",
    "periods = {\"1\" : period1, \"2\" : period2, \"3\":period3, \"4\":period4}\n",
    "\n",
    "for p in [\"1\", \"2\", \"3\", \"4\"]:\n",
    "\n",
    "    for i in range(0,74):\n",
    "\n",
    "        nps = gpd.read_file(tile_list[i])\n",
    "        nps = nps.to_crs(\"EPSG:4326\")\n",
    "\n",
    "        nps_gc = nps.to_json()\n",
    "        nps_gc = json.loads(nps_gc)\n",
    "\n",
    "        task_name = \"p\" + p + \"_\" + short_name[i][0:7]        # task name = period + tile name\n",
    "        task_type = \"area\"                                    # we are using polygon instead of points to extract data\n",
    "        proj = projs[\"geographic\"][\"Name\"]                    # it MUST BE geographic projection make this api work\n",
    "        outFormat = [\"geotiff\"]\n",
    "        startDate = periods[p][0]\n",
    "        endDate = periods[p][1]\n",
    "        recurring = False\n",
    "\n",
    "        task = {                                # build a json type of geo area to indicate ROI\n",
    "            \"task_type\": task_type,             # area sample task\n",
    "            \"task_name\": task_name,\n",
    "            \"params\": {\n",
    "                \"dates\": [\n",
    "                {\n",
    "                    \"startDate\": startDate,\n",
    "                    \"endDate\": endDate\n",
    "                }],\n",
    "                \"layers\": prodLayer,\n",
    "                \"output\": {\n",
    "                    \"format\": {\n",
    "                        \"type\": outFormat[0]    # geotiff format\n",
    "                    },\n",
    "                    \"projection\": proj\n",
    "                },\n",
    "                \"geo\": nps_gc\n",
    "                }\n",
    "        }\n",
    "\n",
    "\n",
    "        token = token_response['token']                      # Save login token to a variable\n",
    "        head = {'Authorization': 'Bearer {}'.format(token)}  # Create a header to store token information, needed to submit a request\n",
    "\n",
    "        task_response = r.post(\"{}task\".format(api), json = task, headers = head).json()\n",
    "        print(task_response)\n",
    "\n",
    "        task_id = task_response['task_id']                                               # Set task id from request submission\n",
    "        status_response = r.get('{}status/{}'.format(api, task_id), headers=head).json() # Call status service with specific task ID & user credentials\n",
    "        print(status_response['status'])\n",
    "\n",
    "        write_task_id = task_id + \" : \" + task_name + \":\" + status_response['status']\n",
    "        t = open(os.path.join(\"D:/Zekun/Landsat_ARD/Landsat_ARD/se_ard/\", prods,\"_task_id_by_tile.txt\"), 'a')\n",
    "        t.write(write_task_id +\"\\n\")\n",
    "        t.close()\n",
    "\n",
    "        print('orders have been successfully submitted.')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 81,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'task_id': 'ae94c133-aaf2-4668-8988-099541898526', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'd9d804c5-b532-4e94-9c2a-8f56704e75e4', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'e9790ee5-be00-4fd5-ba06-38f24dfcccbc', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ac5d7f3c-c665-4c1e-897e-7b172ddc2f67', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'aabcca8b-5808-4943-a0cc-dadd4c47042e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'dbef8c6e-0277-4b8e-b5e8-e91171653388', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '3ea12482-430b-4185-8d8c-f8747af6e2c0', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '2df2bdeb-5e32-488e-ae2a-4e8078c1d4c5', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'd9543cf1-0387-4c36-8010-4c8e85a95ce3', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '39e0fb74-79e2-4b67-b790-9c54a0ed607b', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '33f85c23-8858-468f-ac58-a971ba3ed095', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'f8a78195-2967-4a05-b881-5273aa77c251', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ac9e0a47-20f5-4422-bed6-2fd0fa7550c3', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '19e87101-c421-4a7b-9087-8bbbfcb64870', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '32d00f97-f050-4a6c-bf6c-d321f7a875c3', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '875b7c26-1937-46e5-b8d5-4d1c5ae9a040', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '1f31941e-052b-45d1-9189-3a1e9b9eda3e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '357e5d42-a5ee-4645-a666-4acaeb16d124', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '53bfc740-70cc-4e26-b790-09cd8bed2080', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '6ed97544-254e-4547-87d0-50b4979586fe', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '57107b80-5c90-42a6-a470-db2d74d50426', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '986f036e-d295-4101-bed4-4f0e8463b798', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '10fd792d-c4dc-4b38-add9-ae95d9d0339f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '6fea529c-ad65-40eb-bbdb-b57eb7705269', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'fbfe877a-8bc8-4311-859c-84e7efd41042', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '61ca8609-4116-46f2-8c80-9f283fcd76d5', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '97735176-c716-47be-bdc0-1c05ae339a26', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'a93db366-0713-4669-bedc-d1ceb66328ba', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '29b4cc33-4cde-4e59-8dcb-5e95d50d7f0c', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'baea0fd4-7173-418b-9ba5-4b81e960e428', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '16315538-6818-4b10-85a1-1e9ddf0aa199', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'b650415e-27cb-4c7e-881d-2c75a27cb803', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'd97e9210-c3d6-42ea-b621-82533bb2678b', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '71728f96-ec74-433a-a5a4-c8e55e587328', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'adac063a-81db-458c-8dca-53cfc83cb6ca', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '26d66aac-ba9b-4e75-9011-475ceec41eac', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'f4b17590-8f73-4561-88d5-7380cbd79c69', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '4b81e47f-05d7-470d-b754-9e9d0987e762', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '09416a97-60b1-4389-aa98-4fdba573fa8c', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'bcc8c5e3-c83b-498e-a2c1-671ec6665d7b', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '2d09cc49-353c-416a-9eac-4bd4d698cdda', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'f2b64cc1-0ba6-4990-b116-bf7a29db2062', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'a7e2f72f-5e90-47f2-a335-451e654bb81b', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'd47690d4-bef6-453f-8a5e-9c33b5b3e16f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ce44e7e4-4174-49bb-b277-b4a9ac6d26b9', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'e421abcb-3f3b-4d57-b7c3-b08e334b4b50', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '2335a2d7-518c-4e32-aa7c-0141a600188a', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '34781204-2d44-4775-ab80-82a73aa9bf11', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '82a0a2b7-6ceb-436e-a410-fe35c7af13a6', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ee54c9dc-3059-4f1d-8169-88ce9e09cced', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '808e339f-a839-4bf4-9d15-e583f3b8a847', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'b6d2b513-b18b-4cb0-9e00-8c982a9ace12', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'e4aaf470-593e-4a5e-85a3-c1ee59e9f266', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'bd52c759-3471-4040-9f94-e06a83b3a0aa', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '0886683c-f97a-4f52-9a56-78f97be5e955', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '3ed712f3-39d6-47ae-92a2-ba2fb4933871', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '96d1ae0e-19ec-4f17-9fd7-487b09518589', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '1f9eb233-2cd7-4e9e-a114-37297f833f15', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '4de417dc-9ff5-4cbe-b41b-2ffc02365d33', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ce280b8c-a5ec-4769-85d0-f8c3b2dc12a2', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '775bc31e-d5c9-4647-8642-01c6b5a99ead', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '538b0c7b-0df1-4a5b-a77f-ef36f25bedeb', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'c541e9e9-0258-4bb8-be7f-05bebcd5e081', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'f49f9097-7efa-4e6d-932c-04c5573031f2', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '274be638-b4a3-4cd8-aabe-8c54ea7437f8', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'dd7c61a8-144f-432c-b62a-c13f80d4430e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'c380fece-e6dd-4c13-ba26-df25105ff249', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '266cfb52-f194-4c45-81af-0a536e2131a4', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ec6e2b51-1e55-4996-9c71-084a7071368a', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'e1a7b9f5-14c8-4f78-b913-82d5e4490f9e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '51359bf4-24a1-48d1-a62a-3b759392d794', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '2c17cd6a-a95c-4560-9db4-003682f2b62f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '6001f901-0dc9-4351-a5cc-5bdefcfd4dab', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '75bd36b9-d018-4ad9-98ef-26332df9af4d', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'b6e6eb65-e73b-402a-9e73-4e9a49c83386', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'de3360c8-03fe-494b-9ff7-44c3badf55ed', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'e568e7d4-965b-4a06-9bfc-acc492e7bbde', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'f624dfc3-9ab9-4db4-839d-179658ac467c', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'd6c529dc-65e5-40fa-8e5b-b5f1a088a91e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '4ba92153-ff17-463b-a060-7c3f0db1840f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ee659ff1-8a77-42c7-8299-9e992ae2a749', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '830d2908-3ea4-4300-bc79-3339b31fa10e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '097ba65a-7aa0-4402-9d19-85d379f0fe5e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'ebcea5c9-bad7-41ca-84a4-5d22f13e554c', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'bf1599ae-9c17-45e4-af67-5a0bec498e14', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '2db7a300-88fd-40b4-83ff-4abd84f27a7f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'fd93bebb-5401-4d64-a110-7ee901981a10', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '703beb26-8d36-4b37-90a7-925529a89874', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '7910bfd6-93ff-4bca-be05-f35dd6fee76d', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '4fbaba4a-6de0-43ae-b20e-5fa05c89635f', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '74790063-406b-4785-9ac7-8c2d53368267', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '99d4b366-22de-4438-8e60-60432d38abf7', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '665d1cbb-ee33-4f9f-8e9e-8e03dbf34445', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '17bb3eae-9b44-4ba5-a3ee-83f1a0a8e77a', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '85494de9-7516-4065-aa7f-621951b71bd6', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '6e809ebd-2bac-49c4-bb03-59e5cfe3d509', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '852148a9-2044-4820-9159-3403f4cdbf8e', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '65daaac8-c2f3-4886-af16-63f1e5d52455', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': '963905ba-c803-48ab-8b85-725f9f70a5a2', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'task_id': 'b7932ae0-83d9-42cb-9402-06677d9f7aab', 'status': 'pending'}\n",
      "pending\n",
      "orders have been successfully submitted.\n",
      "{'message': 'You have exceeded the maximum number of requests that are pending or processing.  Please wait until some requests have processed before submitting more.'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'task_id'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-3b067af1465f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtask_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'task_id'\u001b[0m\u001b[1;33m]\u001b[0m                                               \u001b[1;31m# Set task id from request submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mstatus_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}status/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call status service with specific task ID & user credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'task_id'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83e69d90-8bd3-4314-b125-5cace2bfbbe9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' \\nfor x in range(len(task_id_list)):\\n    submited_id = task_id_list[x][0:36]\\n    for y in range(len(task_response)):\\n        if task_response[y][\\'task_id\\'] == submited_id:\\n            if task_response[y][\\'status\\'] == \\'done\\':\\n                download_this_name = task_response[y][\\'task_name\\']\\n                download_this_id = task_response[y][\\'task_id\\']\\n                bundle = r.get(\\'{}bundle/{}\\'.format(api, download_this_id)).json() # Call API and return bundle contents for the task_id as json\\n                #print(bundle[\\'files\\'])\\n                files = {}                                                         # Create empty dictionary\\n                for f in bundle[\\'files\\']: files[f[\\'file_id\\']] = f[\\'file_name\\']    # Fill dictionary with file_id as keys and file_name as values\\n\\n                destDir = os.path.join(\"D:\\\\Zekun\\\\Landsat_ARD\\\\Landsat_ARD\\\\se_ard\", prods[0], download_this_name)\\n                if not os.path.exists(destDir) : os.makedirs(destDir)\\n\\n                for f in files:\\n                    dl = r.get(\\'{}bundle/{}/{}\\'.format(api, download_this_id, f), stream = True)                      # Get a stream to the bundle file\\n                    filename = os.path.basename(cgi.parse_header(dl.headers[\\'Content-Disposition\\'])[1][\\'filename\\'])   # Parse the name from Content-Disposition header\\n                    filepath = os.path.join(destDir, filename)                                                        # Create output file path\\n                    with open(filepath, \\'wb\\') as f:                                                                   # Write file to dest dir\\n                        for data in dl.iter_content(chunk_size=8192): f.write(data)\\n                print(\\'Downloaded files can be found at: {}\\'.format(destDir))\\n\\n    else: pass '"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "## establish credendtial for the api\n",
    "import requests as r\n",
    "import getpass, pprint, time, os, cgi, json\n",
    "import geopandas as gpd\n",
    "\n",
    "## Enter Earth Data login Credentials\n",
    "username = \"ZekunLin\"#getpass.getpass('Earthdata Username:')\n",
    "password = \"P6Fn8T47\"#getpass.getpass('Earthdata Password:')\n",
    "\n",
    "api = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  # Set the AρρEEARS API to a variable\n",
    "\n",
    "token_response = r.post('{}login'.format(api), auth=(username, password)).json() # Insert API URL, call login service, provide credentials & return json\n",
    "del username, password                                                           # Remove user and password information\n",
    "token_response \n",
    "\n",
    "token = token_response['token']                                                     # retrieve the existed orders in the Appeears\n",
    "response = r.get('https://lpdaacsvc.cr.usgs.gov/appeears/api/task', \n",
    "    headers={'Authorization': 'Bearer {0}'.format(token)})\n",
    "task_response = response.json()\n",
    "\n",
    "t = open(\"D:/Zekun/Landsat_ARD/Landsat_ARD/se_ard/LT5_task_id_by_tile.txt\", \"r\")    # read the list of orders\n",
    "task_id_list = t.read().split('\\n')\n",
    "\n",
    "\n",
    "for x in range(len(task_id_list)):\n",
    "    submited_id = task_id_list[x][0:36]\n",
    "    for y in range(len(task_response)):\n",
    "        if task_response[y]['task_id'] == submited_id:\n",
    "            if task_response[y]['status'] == 'done':\n",
    "                download_this_name = task_response[y]['task_name']\n",
    "                download_this_id = task_response[y]['task_id']\n",
    "                bundle = r.get('{}bundle/{}'.format(api, download_this_id)).json() # Call API and return bundle contents for the task_id as json\n",
    "                #print(bundle['files'])\n",
    "                files = {}                                                         # Create empty dictionary\n",
    "                for f in bundle['files']: files[f['file_id']] = f['file_name']    # Fill dictionary with file_id as keys and file_name as values\n",
    "\n",
    "                destDir = os.path.join(\"D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\", prods[0], download_this_name)\n",
    "                if not os.path.exists(destDir) : os.makedirs(destDir)\n",
    "\n",
    "                for f in files:\n",
    "                    dl = r.get('{}bundle/{}/{}'.format(api, download_this_id, f), stream = True)                      # Get a stream to the bundle file\n",
    "                    filename = os.path.basename(cgi.parse_header(dl.headers['Content-Disposition'])[1]['filename'])   # Parse the name from Content-Disposition header\n",
    "                    filepath = os.path.join(destDir, filename)                                                        # Create output file path\n",
    "                    with open(filepath, 'wb') as f:                                                                   # Write file to dest dir\n",
    "                        for data in dl.iter_content(chunk_size=8192): f.write(data)\n",
    "                print('Downloaded files can be found at: {}'.format(destDir))\n",
    "\n",
    "    else: pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h18_v17.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h19_v17.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h20_v17.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v10.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v11.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h21_v17.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v10.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v11.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h22_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v10.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v11.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h23_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v10.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v11.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h24_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v10.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v11.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v12.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v13.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v14.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v15.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v16.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v17.shp\nD:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles\\h25_v18.shp\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "tile_list = glob.glob(\"D:/Zekun/Landsat_ARD/Landsat_ARD/ARD_Uni_shapefiles/*.shp\")      # where the 75 tiles stored\n",
    "for i in range(5, 56):\n",
    "    print(tile_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' bundle = r.get(\\'{}bundle/{}\\'.format(api,task_id)).json()  # Call API and return bundle contents for the task_id as json\\n\\n\\nfiles = {}                                                       # Create empty dictionary\\nfor f in bundle[\\'files\\']: files[f[\\'file_id\\']] = f[\\'file_name\\']   # Fill dictionary with file_id as keys and file_name as values\\n\\n\\ndestDir = os.path.join(\"D:\\\\Zekun\\\\Landsat_ARD\\\\Landsat_ARD\\\\se_ard\\\\CU_LT05.001\", task_name)\\nif not os.path.exists(destDir) : os.makedirs(destDir)\\n\\nfor f in files:\\n    dl = r.get(\\'{}bundle/{}/{}\\'.format(api, task_id, f), stream=True)           # Get a stream to the bundle file\\n    filename = os.path.basename(cgi.parse_header(dl.headers[\\'Content-Disposition\\'])[1][\\'filename\\'])  # Parse the name from Content-Disposition header \\n    filepath = os.path.join(destDir, filename)                                                       # Create output file path\\n    with open(filepath, \\'wb\\') as f:                                                                  # Write file to dest dir\\n        for data in dl.iter_content(chunk_size=8192): f.write(data) \\nprint(\\'Downloaded files can be found at: {}\\'.format(destDir)) '"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "\n",
    "bundle = r.get('{}bundle/{}'.format(api,task_id)).json()  # Call API and return bundle contents for the task_id as json\n",
    "\n",
    "\n",
    "files = {}                                                       # Create empty dictionary\n",
    "for f in bundle['files']: files[f['file_id']] = f['file_name']   # Fill dictionary with file_id as keys and file_name as values\n",
    "\n",
    "\n",
    "destDir = os.path.join(\"D:\\Zekun\\Landsat_ARD\\Landsat_ARD\\se_ard\\CU_LT05.001\", task_name)\n",
    "if not os.path.exists(destDir) : os.makedirs(destDir)\n",
    "\n",
    "for f in files:\n",
    "    dl = r.get('{}bundle/{}/{}'.format(api, task_id, f), stream=True)           # Get a stream to the bundle file\n",
    "    filename = os.path.basename(cgi.parse_header(dl.headers['Content-Disposition'])[1]['filename'])  # Parse the name from Content-Disposition header \n",
    "    filepath = os.path.join(destDir, filename)                                                       # Create output file path\n",
    "    with open(filepath, 'wb') as f:                                                                  # Write file to dest dir\n",
    "        for data in dl.iter_content(chunk_size=8192): f.write(data) \n",
    "print('Downloaded files can be found at: {}'.format(destDir)) "
   ]
  }
 ]
}